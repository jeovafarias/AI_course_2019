{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Neural networks with Gradient Descent\n",
    "So, today, that's it, we are trying it... Gradient Descent for Deep Learning! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Cross entropy\n",
    "Given two distributions $p, q \\in \\mathbb{R}^n$, create a function that computes their cross entropy $\\operatorname{CrossEntropy}(p, q)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cross_entropy(p, q):\n",
    "    return # Your code goes here\n",
    "\n",
    "p = [.5, .2, .3]\n",
    "q = [.3, .3, .4]\n",
    "\n",
    "print(cross_entropy(p, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the way we are going to work here we need an **one-hot** representation of the classes (Remember the exercises on our third day?). How do you do it? Create a function ``one_hot()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    y = np.array(y).astype(np.int)\n",
    "    # Your code goes here\n",
    "    return Y\n",
    "\n",
    "y = [0, 1, 2, 1]\n",
    "print(one_hot(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find the Cross Entropy Loss of a classification $y$ and the output of a softmax $O$. Remember to use the ``one_hot()`` function as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(Y, O):\n",
    "    # Your code goes here\n",
    "    return answer\n",
    "    \n",
    "y = [1, 2, 0]\n",
    "O = np.array([[0.2, 0.7, 0.1], [0.05, 0.1, 0.85], [0.98, 0.02, 0.02]]).T\n",
    "Y = one_hot(y)\n",
    "print(cross_entropy_loss(Y, O))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 One Layer - Multiclass perceptron\n",
    "![a](images/nn_1layer.png)\n",
    "First of all, remember the codes you had for ``get_datasets()``, ``add_bias()`` and ``softmax()``? Bring them here :) For now, we'll be dealing with the ``data_3classes.txt`` dataset. Also, set ``ratio = .7``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(A):\n",
    "    expA = np.exp(A)  \n",
    "    return expA / np.sum(expA, axis=0, keepdims=True)\n",
    "\n",
    "# Your code goes here\n",
    "\n",
    "data, ratio = np.loadtxt('data/data_3classes.txt'), .7\n",
    "X_train, X_test, classes_train, classes_test = get_datasets(data, ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as we discussed in class, this gradient descent algorithm has two phases: *feed-forward* and *back-propagation*. Let's make a function for each that takes into account the *whole* dataset (it's ok if you find it hard, it's the most difficult part of the today!). \n",
    "\n",
    "Let's begin with feed forward. We know that for one vector $x \\in \\mathbb{R^2}$, this step is simply $o = \\text{softmax}(Wx)$. How would you do it for the whole dataset $X \\in \\mathbb{R}^{70 \\times 2}$? Notice that the softmax function will be applied to a whole matrix in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(W, X):\n",
    "    # Your code goes here\n",
    "    return O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, back propagation! What did we see in class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(X, Y, O):\n",
    "    # Your code goes here\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, initialize the variables we'll be optimizing (i.e, that will be part of the gradient descent algorithm). We have ``W`` which is a matrix $3 \\times 2$ and ``w0`` which is a vector of 3 elements. Also initialize the learning rate ``eta`` to .01. Repeat the gradient descent for about 1000 epochs and compute the loss at each epoch. Save the loss in a vector and plot it at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data, ratio = np.loadtxt('data/data_3classes.txt'), .7\n",
    "X_train, X_test, classes_train, classes_test = get_datasets(data, ratio)\n",
    "\n",
    "nc = 3\n",
    "eta = .01\n",
    "n_epochs = 10000;\n",
    "loss = []\n",
    "\n",
    "# Your code goes here\n",
    "\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a function with ``gradient_descent_nn`` (for neural network) that receives the train data and classes and outputs a matrix of weights ``W``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_nn(X_train, classes_train):\n",
    "    # Your code goes here\n",
    "    return W\n",
    "\n",
    "data, ratio = np.loadtxt('data/data_3classes.txt'), .7\n",
    "X_train, X_test, classes_train, classes_test = get_datasets(data, ratio)\n",
    "\n",
    "W = gradient_descent_nn(X_train, classes_train)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, how do you classify all the points using ``W`` you computed previously? Hint.: The function is extremely similar to ``feed_forward()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(W, X):\n",
    "    # Your code goes here\n",
    "    return cla\n",
    "\n",
    "data, ratio = np.loadtxt('data/data_3classes.txt'), .7\n",
    "X_train, X_test, classes_train, classes_test = get_datasets(data, ratio)\n",
    "\n",
    "W = gradient_descent_nn(X_train, classes_train)\n",
    "cla = classify(W, X_test)\n",
    "\n",
    "print(cla)\n",
    "print(classes_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, if you got here, congratulations! The rest of the course will be easy peasy now :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Extra) Two layers - Deep Net!\n",
    "![a](images/nn_2layer.png)\n",
    "Well, it turns out that doing Gradient Descent on the network above is very complicated. I won't make you guy's try to code it yourself... But here's what it would look like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll give this code to you tomorrow, if you want to"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
