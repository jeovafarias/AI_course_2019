{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Multiclass with the Perceptron\n",
    "Using the perceptron as an example of Binary Classifier, we can do multiclass classification! How? Let's see... First of all, bring here the code you had from yesterday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fit_line_dataset(x, y, data):\n",
    "    line = np.stack((x, y), axis=1)\n",
    "    flags = np.all(np.logical_and(line < np.max(data, axis=0)[0:2], line > np.min(data, axis=0)[0:2]), axis=1)\n",
    "    line = line[flags, :]\n",
    "    new_x = line[:, 0]\n",
    "    new_y = line[:, 1]\n",
    "    # Plot these returns in your code \"plot_data(data, w)\"\n",
    "    return new_x, new_y\n",
    "\n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Multi-classes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were talking about two classes since Wednesday. It happens that our problem might present more classes than that! In this section, we'll expand the perceptron algorithm to more than two classes! Our new dataset is called ``data_3classes.txt``. It is very similar to ``data.txt``, but now the last column of it takes values from 0 to 2 (therefore three classes). Let's first create a function to visualize this data (in this function, we'll forget about the markers and the line and we'll differentiate the classes by colors).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_3classes(X, classes):\n",
    "    # Your code goes here\n",
    "    plt.show()\n",
    "    \n",
    "data = np.loadtxt('data/data_3classes.txt')\n",
    "X, classes = preprocess_data(data)\n",
    "plot_data_3classes(X, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is your job to code the *one-vs-rest* strategy for multiclass classification as discussed in class. Make use of the perceptron function you coded previously and store the classifiers (i.e. the $w$'s) in the rows of a matrix called ``W``. Assume that you know that there are 3 classes in your dataset beforehand. Also, make a function ``replace_numbers(v, n)`` that gets a vector ``v`` and a number ``n`` and makes a new ``v`` such that ``new_v[i]`` is ``1``, if ``v[i] == n`` or it is ``-1``, otherwise (you coded this in a HW previously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numbers(v, n):  \n",
    "    # Your code goes here\n",
    "    return v_new\n",
    "\n",
    "def perceptron_3classes(X, classes):\n",
    "    n_classes = 3\n",
    "    n, d = np.shape(X)\n",
    "    W = np.zeros((n_classes, d + 1))  # The '+1' is due to the bias that you'll add in the perceptron alg.\n",
    "    # Your code goes here \n",
    "    return W\n",
    "    \n",
    "data = np.loadtxt('data/data_3classes.txt')\n",
    "X, classes = preprocess_data(data)\n",
    "ws = perceptron_3classes(X, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, a result like that is quite boring... Now, improve the function ``plot_data_3classes()`` to also plot the lines that are defined by the ``w`` for each class. Hint.: First plot the points and then the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def plot_data_3classes_w_lines(data, ws):\n",
    "    # Your code goes here\n",
    "    plt.show()\n",
    "        \n",
    "data = np.loadtxt('data/data_3classes.txt')\n",
    "X, classes = preprocess_data(data)\n",
    "W = perceptron_3classes(X, classes)\n",
    "plot_data_3classes_w_lines(data, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our task will be to do training and testing! What to do now? Let's get the score for each points with respect to each line/$w$. Can you compute this matrix of scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, ratio = np.loadtxt('data/data_3classes.txt'), .7\n",
    "X_train, X_test, classes_train, classes_test = get_datasets(data, ratio)\n",
    "W = perceptron_3classes(X_train, classes_train)\n",
    "\n",
    "n, d = np.shape(X_test)\n",
    "scores = np.zeros((3, n)) \n",
    "classification = np.zeros((3, n)) \n",
    "\n",
    "# Your code goes here\n",
    "\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these numbers mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Dealing with ambiguities\n",
    "In the previous result (if you used ``ratio = .7``), what do you think we should do about the ambiguous classification? What to do in that case? First, modify your ``classify(x, w)`` function to return the class *and* the dot product (call this last value ``score``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(x, w):\n",
    "    # Your code goes here\n",
    "    return cla, score\n",
    "\n",
    "def classify_all_points(X, w):\n",
    "    # Your code goes here\n",
    "    return classification, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, redo the classification about and compute the scores along with the classifications. Print the matrix of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/data_3classes.txt')\n",
    "X_train, X_test, classes_train, classes_test = get_datasets(data, ratio)\n",
    "W = perceptron_3classes(X_train, classes_train)\n",
    "\n",
    "n, d = np.shape(X_test)\n",
    "scores = np.zeros((3, n)) \n",
    "# Your code goes here\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how do we compute the index of the maximum score for each point when classified by each line? In other words, which lines classifies the best each point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/data_3classes.txt')\n",
    "X_train, X_test, classes_train, classes_test = get_datasets(data, ratio)\n",
    "W = perceptron_3classes(X_train, classes_train)\n",
    "\n",
    "n, d = np.shape(X_test)\n",
    "scores = np.zeros((3, n)) \n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the loss and the accuracy now? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 The softmax function and soft classification\n",
    "Previously, we simply computed the maximum index among the classification scores. What if we could actually have a better idea of how certain a point is of being classified in a given class?\n",
    "\n",
    "In order to do that, we'll use the softmax function of a vector as explained in class. Make a function ``softmax()`` that receives a matrix $M$ and compute the softmax of each of its columns (if you're are finding it complicated, it's ok! Ask the instructor if you need help!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(M):\n",
    "    # Your code goes here\n",
    "    return soft_max_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, simply pass the scores you've gotten previously and compute the softmax of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/data_3classes.txt')\n",
    "ratio = .7\n",
    "X_train, X_test, classes_train, classes_test = get_datasets(data, ratio)\n",
    "W = perceptron_3classes(X_train, classes_train)\n",
    "\n",
    "# Your code goes here\n",
    "\n",
    "soft_max_scores = softmax(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, take a time to see what ``soft_max_scores`` is saying (actually, print the transpose of ``soft_max_scores`` as ``soft_max_scores.T`` in order to visualize it better).\n",
    "Lastly, compute the index of maximum softmax score of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Using Matrix Multiplication\n",
    "Ok, so far we had to use more loops than necessary and, honestly, that's not very efficient, even computationally (it gets the code slower). So far, I've been making you guys code an unnecessarily long classification step, when it could have being done in *one* line of code. Let's get there! \n",
    "\n",
    "Firstly, remembering what we discussed about matrix multiplication, how can I change the function ``classify_all_points()`` so it receives a matrix $W$ and the datapoints ``X`` and does all the dot products of each $x[i]$ and the perceptron lines at once (call this new function ``classify_all_points_MM_v1(W, X)``)? Hint.: This will be using multiplications of matrices and vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(x, w):\n",
    "    # Your code goes here\n",
    "    return cla, score\n",
    "\n",
    "def classify_all_points_MM_v1(W, X):\n",
    "    # Your code goes here  \n",
    "    return classification\n",
    "\n",
    "data, ratio = np.loadtxt('data/data_3classes.txt'), .7\n",
    "X_train, X_test, classes_train, classes_test = get_datasets(data, ratio)\n",
    "W = perceptron_3classes(X_train, classes_train)\n",
    "\n",
    "classification_scores = classify_all_points_MM_v1(W, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Challenge) Now, there is still a for loop in ``classify_all_points_MM_v1(W, X)``. How can I get rid of it with a matrix multiplication between two matrices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(x, w):\n",
    "    # Your code goes here\n",
    "    return cla, score\n",
    "\n",
    "def classify_all_points_MM_v2(W, X):\n",
    "    # Your code goes here   \n",
    "    return classification\n",
    "\n",
    "data, ratio = np.loadtxt('data/data_3classes.txt'), .7\n",
    "X_train, X_test, classes_train, classes_test = get_datasets(data, ratio)\n",
    "W = perceptron_3classes(X_train, classes_train)\n",
    "\n",
    "classification_scores = classify_all_points_MM_v2(W, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 (Extra) Non Linearly separable datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 The XOR Function\n",
    "In logic, there is a very important function that is also very simple. You have two binary (either 0 or 1) inputs $x$, $y$ and it returns $1$ if $x = y$ or $-1$ if $x \\neq y$. It's name is XOR. Given the following dataset (that represents all possible XOR inputs/outputs), draw its scatter plot as we did yesterday:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "classes = np.array([1, -1, -1, 1])\n",
    "\n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your mind, can you find a straight line that separates pluses and minuses? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 The Moons dataset\n",
    "In ``/data`` there is a dataset called ``moons.txt``. Can you also plot it? After doing that, can you find a line that separates the pluses  from the minuses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is now your definition of Non Linearly separable dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.3 The MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's just check our future data, the MNIST dataset. It's a non linearly separable database of handwritten digits from 0 to 9 (therefore, 10 classes). For now, let's simply check it out. The file ``mnnist_small.txt`` contains a small part of the whole original dataset and it follows the dataset format that we've been working with. Let's check it out. Just print one of the points of it (remember that the last element is the class the digit belongs to). Call this point ``p``. Also, print the classification of that point and store it in a variable ``c``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/mnnist_small.txt')\n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector that you just printed is a handwritten digit! We can't see a lot from it, can we? That's because we need to convert the vector in a matrix and then *plot the matrix itself*! You can plot matrices with the function ``plt.imshow()`` from ``matplotlib.pyplot``.In order to convert the vector to a matrix, you need to reshape it with ``np.reshape()``.\n",
    "\n",
    "Now type ``np.reshape(p, (28, 28))``, followed by ``np.imshow(m)`` and then ``plt.show()``. Is that what the classification ``c`` tells you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing, what does this plot of a matrix tell you about *images* in general?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
